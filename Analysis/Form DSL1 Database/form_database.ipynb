{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DSL1 Database Notebook\n",
    "\n",
    "In this notebook are presented the functions used to form a DSL1 database from the output json from the [crawler](https://github.com/R2-P2/wf-crawler), a json ('wf_crawl_nextflow.json') is already present in same file as this notebook. \n",
    "\n",
    "This notebook also produces 2 new json files :\n",
    "\n",
    "- 'json_nextflow_DSL.json' this json contains the same information as 'wf_crawl_nextflow.json' with the added information of which version of DSL (DSL1 or DSL2) is used in the repository\n",
    "- 'json_nextflow_DSL1.json' contains exclusively the repository written in DSL1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To form the database, for each github repository (project), the nextflow (.np extension) files are retrieved at the root. Since we are only interested in retrieving the DSL1 workflows (which are written in a single file), to simplify the search the root is only searched (generally the main is placed at the root anyway). \n",
    "\n",
    "The next step is to define if the repository is written in DSL1 or DSL2 : \n",
    "- if all the files are written in DSL1, the repository is defined as written in DSL1 and we can add each (nextflow from the repository) file to the database as a single workflow\n",
    "- if a single file is written in DSL2, the repository is defined as written in DSL2 and we do not add the remaining files to the database (reminder a DSL2 workflow is decomposed into multiple file, but the version of nextflow is only defined in the main of workflow, which is placed at the root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import urllib\n",
    "import re\n",
    "from urllib.request import urlopen\n",
    "import glob\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the addresses used to save the database, at the end of the execution of the notebook : \n",
    "\n",
    "- __address_saved_DSL1__ will contain only the workflows written in DSL1\n",
    "- __address_saved_DSL2__ will contain the remaining workflows which we are not sure of the version since in the repository was found a file written in DSL2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "address_saved_DSL1 = 'some/addresse'\n",
    "address_saved_DSL2 = 'some/addresse'\n",
    "os.system(f\"mkdir -p {address_saved_DSL1}\")\n",
    "os.system(f\"mkdir -p {address_saved_DSL2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the initial json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_data={}\n",
    "with open('wf_crawl_nextflow.json') as json_file:\n",
    "    #Read the file\n",
    "    json_data = json.load(json_file)\n",
    "    json_data.pop('last_date', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function that retrieves the links of the nextflow file at the root of the project\n",
    "\n",
    "Since DSL1 workflows are written in just one file and are generally placed at the root of the project\n",
    "\n",
    "Doing this is restrictive : but doing this we only retrieve workflows which we are sure are written in DSL1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_links_to_download(id=\"joshua-d-campbell/nf-GATK_Exome_Preprocess\"):\n",
    "    try:\n",
    "        #Step 1 : Retrieve HTML of Web page\n",
    "        fp = urllib.request.urlopen(\"https://github.com/\"+id)\n",
    "        html = fp.read()\n",
    "        html = html.decode(\"utf8\")\n",
    "        fp.close()\n",
    "        #Step 2 : Retrieve the links of the nextflow files in the project\n",
    "        pattern = r'href=\"(.+\\.nf)\"'\n",
    "        links=[]\n",
    "        for match in re.finditer(pattern, html):\n",
    "            links.append(match.group(1))\n",
    "        #Step 3 : Retrieve the raw addresses of the nextflow file to be able to download them later\n",
    "        links_to_download=[]\n",
    "        for l in links:\n",
    "            fp = urllib.request.urlopen(\"https://github.com/\"+l)\n",
    "            html = fp.read()\n",
    "            html = html.decode(\"utf8\")\n",
    "            fp.close()\n",
    "            pattern = r' href=\"(.+\\.nf)\" id=\"raw-url\"'\n",
    "            for match in re.finditer(pattern, html):\n",
    "                links_to_download.append(match.group(1))\n",
    "        return links_to_download\n",
    "    except Exception as inst:\n",
    "        print(inst)   \n",
    "        return []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions that download the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_files(address, data):\n",
    "    i=0\n",
    "    for id in data:\n",
    "        i+=1\n",
    "        print(id, f'{i}/{len(data)}')\n",
    "        links= get_links_to_download(id)\n",
    "        nb=0\n",
    "        for l in links:\n",
    "            nb+=1\n",
    "            link= f'https://github.com{l}'\n",
    "            try:\n",
    "                f = urlopen(link)\n",
    "                myfile = f.read().decode('utf-8')\n",
    "                myText = open(f'{address}/{id.replace(\"/\", \"__\")}_{nb}.nf','w')\n",
    "                myText.write(myfile)\n",
    "                myText.close()\n",
    "            except Exception as inst:\n",
    "                print(inst)\n",
    "                None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function that checks if there is presence of the DSL2 indicator in the file and defines if a repository is considered to be written in DSL1 or not "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_DSL1(address, data):\n",
    "    for j in data:\n",
    "        files= []\n",
    "        for file in glob.glob(address+f'/{j.replace(\"/\", \"__\")}_*.nf'):\n",
    "            files.append(file)\n",
    "        if(len(files)!=0):\n",
    "            DSL1 = True\n",
    "            for f in files:\n",
    "                txt = Path(f).read_text()\n",
    "                if(bool(re.compile(r\"(nextflow\\.(preview|enable)\\.dsl\\s*=\\s*2)\").search(txt))):\n",
    "                    DSL1 = False\n",
    "            data[j]['DSL1'] = DSL1\n",
    "        else:\n",
    "            data[j]['DSL1'] = None\n",
    "\n",
    "    with open('json_nextflow_DSL.json', 'w') as fp:\n",
    "        json.dump(data, fp,  indent=4)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function that extract the dictionnary containning only the DSL1 repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_DSL1(dict):\n",
    "    df = pd.DataFrame.from_dict(dict).T\n",
    "    data = df[df['DSL1']==True].T.to_dict()\n",
    "    with open('json_nextflow_DSL1.json', 'w') as fp:\n",
    "        json.dump(data, fp,  indent=4)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function that moves the DSL1 workflows to their corresponding directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def move_DSL1(old_address, new_address, dict_DSL1):\n",
    "    for j in dict_DSL1:\n",
    "        for file in glob.glob(old_address+f'/{j.replace(\"/\", \"__\")}_*.nf'):\n",
    "           os.system(f'mv {file} {new_address}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main which links all the functions together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_files(address_saved_DSL2, json_data)\n",
    "json_data_DSL = check_DSL1(address_saved_DSL2, json_data)\n",
    "data = extract_DSL1(json_data_DSL)\n",
    "move_DSL1(address_saved_DSL2, address_saved_DSL1, data)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
